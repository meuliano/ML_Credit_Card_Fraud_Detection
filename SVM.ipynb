{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection\n",
    "#### By Shadi Bavar, Matthew Euliano, and Claire Parisi\n",
    "\n",
    "\n",
    "##### Importing Required Libraries & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report, f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features:  31\n",
      "Number of Fraudulent Transactions:  492 (0.17%)\n",
      "Number of Normal Transactions 284315 (99.83%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Data\n",
    "data = pd.read_csv('creditcard.csv')\n",
    "\n",
    "# Get samples for each class\n",
    "fraudsDF = data[data.Class == 1]\n",
    "normalDF = data[data.Class == 0]\n",
    "N = len(data)\n",
    "\n",
    "print(\"Number of Features: \", fraudsDF.shape[1])\n",
    "print(\"Number of Fraudulent Transactions: \",fraudsDF.shape[0], \"({:.2f}%)\".format(100*fraudsDF.shape[0]/N))\n",
    "print(\"Number of Normal Transactions\",normalDF.shape[0], \"({:.2f}%)\".format(100*normalDF.shape[0]/N))\n",
    "\n",
    "#See the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularize Features & Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling amount \n",
    "std_scale = StandardScaler()\n",
    "data['Amount'] = std_scale.fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "data['Time'] = std_scale.fit_transform(data['Time'].values.reshape(-1, 1))\n",
    "# data = data.drop(['Time'], axis=1)\n",
    "\n",
    "#Split dataset into inputs (x) and labels (y)\n",
    "x = data.drop(['Class'], axis = 1)\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break dataset into Training and Testing Sets Representative of the Imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set frauds: 0.17 %\n",
      "Test set frauds: 0.17 %\n"
     ]
    }
   ],
   "source": [
    "#split dataset into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0, stratify = y)\n",
    "\n",
    "#check distributions of data\n",
    "fraud_train_pct = y_train[y_train== 1].value_counts()/len(y_train)*100\n",
    "fraud_test_pct = y_test[y_test == 1].value_counts()/len(y_test)*100\n",
    "\n",
    "print('Training set frauds:', round(fraud_train_pct[1], 2), '%')\n",
    "print('Test set frauds:', round(fraud_test_pct[1], 2), '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersample the Training Set to Balance the Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set frauds: 50.0 %\n",
      "Training set non-frauds: 50.0 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#find instances of fraud in training set\n",
    "train_fraud_indices = np.array(y_train[y_train== 1].index)\n",
    "n_train_fraud = len(train_fraud_indices)\n",
    "train_nonfraud_indices = np.array(y_train[y_train== 0].index)\n",
    "\n",
    "#Randomly select number of non-fraud transactions to match the number of fraud transactions\n",
    "random_indices = np.random.choice(train_nonfraud_indices, n_train_fraud, replace = False)\n",
    "undersample_indices = np.concatenate([train_fraud_indices, random_indices])\n",
    "\n",
    "#Resample the training data\n",
    "x_train_u  = x_train.loc[undersample_indices]\n",
    "y_train_u = y_train.loc[undersample_indices]\n",
    "\n",
    "#Check the new distribution of data\n",
    "fraud_train_pct_u = y_train_u[y_train_u== 1].value_counts()/len(y_train_u)*100\n",
    "nonfraud_train_pct_u = y_train_u[y_train_u== 0].value_counts()/len(y_train_u)*100\n",
    "print('Training set frauds:', round(fraud_train_pct_u[1], 2), '%')\n",
    "print('Training set non-frauds:', round(nonfraud_train_pct_u[0], 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to test and train different SVM Models\n",
    "from sklearn.metrics import classification_report\n",
    "def train_svm_model(x,y,C,kernel,class_weight):\n",
    "    model = SVC(C=C,kernel=kernel,class_weight=class_weight)\n",
    "    model.fit(x, y)\n",
    "    return model\n",
    "    \n",
    "def test_svm_model(x,y,model):\n",
    "    prediction = model.predict(x)\n",
    "    print(confusion_matrix(y,prediction))\n",
    "    # ConfusionMatrixDisplay.from_predictions(y,prediction)\n",
    "    # report= classification_report(y, prediction)\n",
    "    # print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanced Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base SVM Confusion Matrix tested on Training Set\n",
      "[[385   9]\n",
      " [ 27 367]]\n",
      "Base SVM Confusion Matrix tested on Test Set\n",
      "[[55084  1780]\n",
      " [   13    85]]\n"
     ]
    }
   ],
   "source": [
    "# Balanced Model\n",
    "svm_base = train_svm_model(x_train_u,y_train_u,1,'linear','balanced')\n",
    "print(\"Base SVM Confusion Matrix tested on Training Set\")\n",
    "test_svm_model(x_train_u,y_train_u,svm_base)\n",
    "print(\"Base SVM Confusion Matrix tested on Test Set\")\n",
    "test_svm_model(x_test,y_test,svm_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base SVM Confusion Matrix tested on Training Set\n",
      "[[388   6]\n",
      " [ 22 372]]\n",
      "Base SVM Confusion Matrix tested on Test Set\n",
      "[[55063  1801]\n",
      " [   11    87]]\n"
     ]
    }
   ],
   "source": [
    "# Balanced Model\n",
    "svm_base = train_svm_model(x_train_u,y_train_u,1,'linear','balanced')\n",
    "print(\"Base SVM Confusion Matrix tested on Training Set\")\n",
    "test_svm_model(x_train_u,y_train_u,svm_base)\n",
    "print(\"Base SVM Confusion Matrix tested on Test Set\")\n",
    "test_svm_model(x_test,y_test,svm_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base RBF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base SVM Confusion Matrix tested on Training Set\n",
      "[[389   5]\n",
      " [ 42 352]]\n",
      "Base SVM Confusion Matrix tested on Test Set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\meuli\\anaconda3\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55882   982]\n",
      " [   13    85]]\n"
     ]
    }
   ],
   "source": [
    "model = SVC(C=1,kernel='rbf',gamma='scale')\n",
    "model.fit(x_train_u,y_train_u)\n",
    "print(\"Base SVM Confusion Matrix tested on Training Set\")\n",
    "test_svm_model(x_train_u,y_train_u,model)\n",
    "print(\"Base SVM Confusion Matrix tested on Test Set\")\n",
    "test_svm_model(x_test,y_test,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Poly Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base SVM Confusion Matrix tested on Training Set\n",
      "[[387   7]\n",
      " [ 30 364]]\n",
      "Base SVM Confusion Matrix tested on Test Set\n",
      "[[55522  1342]\n",
      " [   13    85]]\n"
     ]
    }
   ],
   "source": [
    "model = SVC(C=10,degree=1,kernel='poly',gamma='scale')\n",
    "model.fit(x_train_u,y_train_u)\n",
    "print(\"Base SVM Confusion Matrix tested on Training Set\")\n",
    "test_svm_model(x_train_u,y_train_u,model)\n",
    "print(\"Base SVM Confusion Matrix tested on Test Set\")\n",
    "test_svm_model(x_test,y_test,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO - Cross-Validation: Hyperparams we can vary = Kernel, C-param, Class Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# C_range = np.logspace(-2, 10, 13)\n",
    "# gamma_range = np.logspace(-9, 3, 13)\n",
    "# param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "# cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "# grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n",
    "# grid.fit(x_train_u, y_train_u)\n",
    "\n",
    "# print(\n",
    "#     \"The best parameters are %s with a score of %0.2f\"\n",
    "#     % (grid.best_params_, grid.best_score_)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.657 (+/-0.078) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.640 (+/-0.099) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.647 (+/-0.060) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.630 (+/-0.082) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.649 (+/-0.063) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.627 (+/-0.086) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.649 (+/-0.063) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.627 (+/-0.084) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.908 (+/-0.055) for {'C': 1, 'kernel': 'linear'}\n",
      "0.913 (+/-0.039) for {'C': 10, 'kernel': 'linear'}\n",
      "0.907 (+/-0.040) for {'C': 100, 'kernel': 'linear'}\n",
      "0.917 (+/-0.043) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       394\n",
      "           1       1.00      0.81      0.89       394\n",
      "\n",
      "    accuracy                           0.90       788\n",
      "   macro avg       0.92      0.90      0.90       788\n",
      "weighted avg       0.92      0.90      0.90       788\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.610 (+/-0.069) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.627 (+/-0.089) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.609 (+/-0.058) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.622 (+/-0.077) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.610 (+/-0.060) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.619 (+/-0.080) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.610 (+/-0.060) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.619 (+/-0.078) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.893 (+/-0.062) for {'C': 1, 'kernel': 'linear'}\n",
      "0.897 (+/-0.048) for {'C': 10, 'kernel': 'linear'}\n",
      "0.891 (+/-0.050) for {'C': 100, 'kernel': 'linear'}\n",
      "0.904 (+/-0.049) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91       394\n",
      "           1       1.00      0.81      0.89       394\n",
      "\n",
      "    accuracy                           0.90       788\n",
      "   macro avg       0.92      0.90      0.90       788\n",
      "weighted avg       0.92      0.90      0.90       788\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Loading the Digits dataset\n",
    "# X, y = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [\n",
    "    {\"kernel\": [\"rbf\"], \"gamma\": [1e-3, 1e-4], \"C\": [1, 10, 100, 1000]},\n",
    "    {\"kernel\": [\"linear\"], \"C\": [1, 10, 100, 1000]},\n",
    "]\n",
    "\n",
    "scores = [\"precision\", \"recall\"]\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, scoring=\"%s_macro\" % score,n_jobs=-1)\n",
    "    clf.fit(x_train_u, y_train_u)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_[\"mean_test_score\"]\n",
    "    stds = clf.cv_results_[\"std_test_score\"]\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_[\"params\"]):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_train_u, clf.predict(x_train_u)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Note the problem is too easy: the hyperparameter plateau is too flat and the\n",
    "# output model is the same for precision and recall with ties in quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1000, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "print(loaded_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\meuli\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\meuli\\Desktop\\ML\\Credit_Card_Fraud_Detection\\FraudDetection.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/meuli/Desktop/ML/Credit_Card_Fraud_Detection/FraudDetection.ipynb#ch0000040?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/meuli/Desktop/ML/Credit_Card_Fraud_Detection/FraudDetection.ipynb#ch0000040?line=23'>24</a>\u001b[0m clf \u001b[39m=\u001b[39m RandomizedSearchCV(SVC(), tuned_parameters, scoring\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_macro\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m score,n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/meuli/Desktop/ML/Credit_Card_Fraud_Detection/FraudDetection.ipynb#ch0000040?line=24'>25</a>\u001b[0m clf\u001b[39m.\u001b[39;49mfit(x_train_u, y_train_u)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/meuli/Desktop/ML/Credit_Card_Fraud_Detection/FraudDetection.ipynb#ch0000040?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest parameters set found on development set:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/meuli/Desktop/ML/Credit_Card_Fraud_Detection/FraudDetection.ipynb#ch0000040?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\meuli\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:926\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    924\u001b[0m refit_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    925\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 926\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    927\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n",
      "File \u001b[1;32mc:\\Users\\meuli\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[LibSVM]\u001b[39m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    254\u001b[0m seed \u001b[39m=\u001b[39m rnd\u001b[39m.\u001b[39mrandint(np\u001b[39m.\u001b[39miinfo(\u001b[39m\"\u001b[39m\u001b[39mi\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mmax)\n\u001b[1;32m--> 255\u001b[0m fit(X, y, sample_weight, solver_type, kernel, random_seed\u001b[39m=\u001b[39;49mseed)\n\u001b[0;32m    256\u001b[0m \u001b[39m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape_fit_ \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32mc:\\Users\\meuli\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:315\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    302\u001b[0m libsvm\u001b[39m.\u001b[39mset_verbosity_wrap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    304\u001b[0m \u001b[39m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[39m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    306\u001b[0m (\n\u001b[0;32m    307\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_,\n\u001b[0;32m    308\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_vectors_,\n\u001b[0;32m    309\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_support,\n\u001b[0;32m    310\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdual_coef_,\n\u001b[0;32m    311\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_,\n\u001b[0;32m    312\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_probA,\n\u001b[0;32m    313\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_probB,\n\u001b[0;32m    314\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_status_,\n\u001b[1;32m--> 315\u001b[0m ) \u001b[39m=\u001b[39m libsvm\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    316\u001b[0m     X,\n\u001b[0;32m    317\u001b[0m     y,\n\u001b[0;32m    318\u001b[0m     svm_type\u001b[39m=\u001b[39;49msolver_type,\n\u001b[0;32m    319\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    320\u001b[0m     class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight_,\n\u001b[0;32m    321\u001b[0m     kernel\u001b[39m=\u001b[39;49mkernel,\n\u001b[0;32m    322\u001b[0m     C\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mC,\n\u001b[0;32m    323\u001b[0m     nu\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnu,\n\u001b[0;32m    324\u001b[0m     probability\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprobability,\n\u001b[0;32m    325\u001b[0m     degree\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdegree,\n\u001b[0;32m    326\u001b[0m     shrinking\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshrinking,\n\u001b[0;32m    327\u001b[0m     tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[0;32m    328\u001b[0m     cache_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache_size,\n\u001b[0;32m    329\u001b[0m     coef0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoef0,\n\u001b[0;32m    330\u001b[0m     gamma\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gamma,\n\u001b[0;32m    331\u001b[0m     epsilon\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepsilon,\n\u001b[0;32m    332\u001b[0m     max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m    333\u001b[0m     random_seed\u001b[39m=\u001b[39;49mrandom_seed,\n\u001b[0;32m    334\u001b[0m )\n\u001b[0;32m    336\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "tuned_parameters = [\n",
    "    {\"kernel\": [\"linear\"], \"C\": [0.1, 1, 10,20,50, 100,200,500, 1000,2000]},\n",
    "]\n",
    "\n",
    "scores = [\"precision\", \"recall\"]\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = RandomizedSearchCV(SVC(),tuned_parameters, n_iter=4, scoring=\"%s_macro\" % score,n_jobs=-1)\n",
    "    clf.fit(x_train_u, y_train_u)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_[\"mean_test_score\"]\n",
    "    stds = clf.cv_results_[\"std_test_score\"]\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_[\"params\"]):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_train_u, clf.predict(x_train_u)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Note the problem is too easy: the hyperparameter plateau is too flat and the\n",
    "# output model is the same for precision and recall with ties in quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17b4c01959b763961e65029bfd8cfa288ed8d0492d779c607a82e02daf2d74ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
