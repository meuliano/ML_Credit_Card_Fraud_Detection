{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Models\n",
    "#### By Shadi Bavar, Matthew Euliano, and Claire Parisi\n",
    "\n",
    "\n",
    "##### Importing Required Libraries & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix,auc,roc_curve,recall_score, f1_score, precision_score\n",
    "\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Data\n",
    "data = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularize Features & Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling amount \n",
    "std_scale = StandardScaler()\n",
    "data['Amount'] = std_scale.fit_transform(data['Amount'].values.reshape(-1, 1))\n",
    "\n",
    "#Split dataset into inputs (x) and labels (y)\n",
    "x = data.drop(['Class'], axis = 1)\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break dataset into Training and Testing Sets Representative of the Imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0, stratify = y)\n",
    "\n",
    "#check distributions of data\n",
    "fraud_train_pct = y_train[y_train== 1].value_counts()/len(y_train)*100\n",
    "fraud_test_pct = y_test[y_test == 1].value_counts()/len(y_test)*100\n",
    "\n",
    "print('Training set frauds:', round(fraud_train_pct[1], 2), '%')\n",
    "print('Test set frauds:', round(fraud_test_pct[1], 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersample the Training Set to Balance the Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find instances of fraud in training set\n",
    "train_fraud_indices = np.array(y_train[y_train== 1].index)\n",
    "n_train_fraud = len(train_fraud_indices)\n",
    "train_nonfraud_indices = np.array(y_train[y_train== 0].index)\n",
    "\n",
    "#Randomly select number of non-fraud transactions to match the number of fraud transactions\n",
    "random_indices = np.random.choice(train_nonfraud_indices, n_train_fraud, replace = False)\n",
    "undersample_indices = np.concatenate([train_fraud_indices, random_indices])\n",
    "\n",
    "#Resample the training data\n",
    "x_train_u  = x_train.loc[undersample_indices]\n",
    "y_train_u = y_train.loc[undersample_indices]\n",
    "\n",
    "#Check the new distribution of data\n",
    "fraud_train_pct_u = y_train_u[y_train_u== 1].value_counts()/len(y_train_u)*100\n",
    "nonfraud_train_pct_u = y_train_u[y_train_u== 0].value_counts()/len(y_train_u)*100\n",
    "print('Training set frauds:', round(fraud_train_pct_u[1], 2), '%')\n",
    "print('Training set non-frauds:', round(nonfraud_train_pct_u[0], 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross Validation for Logisitc Regression Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create function for logisitic regression k_fold cross validation\n",
    "def lr_kfold_cv(x_train,y_train,k):\n",
    "    k_fold = KFold(n_splits = k, shuffle = True, random_state = 4)\n",
    "    c = [0.01, 0.1, 1, 10, 100] #weight values to try\n",
    "    scores = np.zeros(len(c))\n",
    "    for i in range(len(c)):\n",
    "\n",
    "        recall = np.zeros(k)\n",
    "        precision = np.zeros(k)\n",
    "        f1 = np.zeros(k)\n",
    "        indx = 0\n",
    "        for train_index, valid_index in k_fold.split(x_train):\n",
    "            lr = LogisticRegression(C = c[i], penalty= \"l2\", max_iter= 500) #Logistic Regression model\n",
    "            lr.fit(x_train.iloc[train_index], y_train.iloc[train_index]) #Fitting the model\n",
    "            y_pred = lr.predict(x_train.iloc[valid_index]) #Predicting on the validation set\n",
    "\n",
    "            #Evaluating the model\n",
    "            recall[indx] = recall_score(y_train.iloc[valid_index], y_pred)\n",
    "            precision[indx] = precision_score(y_train.iloc[valid_index], y_pred)\n",
    "            f1[indx] = f1_score(y_train.iloc[valid_index], y_pred)\n",
    "            indx += 1\n",
    "\n",
    "        avg_recall = np.mean(recall)\n",
    "        avg_precision = np.mean(precision)\n",
    "        avg_f1 = np.mean(f1)\n",
    "        scores[i] = avg_f1\n",
    "        print('C =', c[i], ', Recall:', avg_recall, 'Precision:', avg_precision, 'F1:', avg_f1)\n",
    "\n",
    "    #Select model with best F1 score\n",
    "    c_val = c[np.argmax(scores)]\n",
    "    print('Select', c_val, 'as the best C value')\n",
    "    return c_val\n",
    "\n",
    "#5 fold CV on Undersampled Dataset to select weight parameter\n",
    "c = lr_kfold_cv(x_train_u, y_train_u, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model with best weight parameter\n",
    "lr = LogisticRegression(C = c, penalty= \"l2\", max_iter= 500) #Logistic Regression model\n",
    "lr.fit(x_train_u, y_train_u) #Fitting the model\n",
    "\n",
    "#Predict on the (skewed) test set\n",
    "y_pred = lr.predict(x_test) \n",
    "\n",
    "\n",
    "#Evaluate the model\n",
    "#Confusion Matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "fruads = y_test[y_test == 1]\n",
    "print('Actual Frauds:', len(fruads))\n",
    "non_fruads = y_test[y_test == 0]\n",
    "print('Actual Non-Frauds:', len(non_fruads))\n",
    "\n",
    "#Calculate metrics\n",
    "pfa = cnf_matrix[0][1]/(len(non_fruads))\n",
    "print('Probability of False Alarm:', pfa)\n",
    "pmd = cnf_matrix[1][0]/(len(fruads))\n",
    "print('Probability of Missed Detection', pmd)\n",
    "trpr = cnf_matrix[1][1]/(len(fruads))\n",
    "print('True Positive Rate:', trpr)\n",
    "\n",
    "#Plot confusion matrix\n",
    "fig = plt.figure()\n",
    "plt.imshow(cnf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['Normal', 'Fraud'])\n",
    "plt.yticks(tick_marks, ['Normal', 'Fraud'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.text(0, 0, '{}'.format(cnf_matrix[0,0]), ha='center', va='center', color='white', fontsize=12)\n",
    "plt.text(1, 0, '{}'.format(cnf_matrix[0,1]), ha='center', va='center', color='black', fontsize=12)\n",
    "plt.text(0, 1, '{}'.format(cnf_matrix[1,0]), ha='center', va='center', color='black', fontsize=12)\n",
    "plt.text(1, 1, '{}'.format(cnf_matrix[1,1]), ha='center', va='center', color='black', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "#Precision and Recall and F1 Score\n",
    "recall = recall_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print('Recall:', recall, 'Precision:', precision, 'F1:', f1)\n",
    "\n",
    "#Plot ROC curve\n",
    "y_pred = lr.fit(x_train_u, y_train_u).decision_function(x_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred, drop_intermediate=False)\n",
    "fig = plt.figure()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot(pfa,trpr, 'ro', label = \"Our Model\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('AUC:', auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model without Data Conditioning (for comparision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use entire (unbalanced) training and testing set to evaluate model\n",
    "lr = LogisticRegression(C = c, penalty= \"l2\", max_iter= 500) #Logistic Regression model\n",
    "lr.fit(x_train,y_train)\n",
    "y_pred = lr.predict(x_test)\n",
    "\n",
    "\n",
    "#Evaluate the model\n",
    "#Confusion Matrix\n",
    "#Evaluate the model\n",
    "#Confusion Matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "fruads = y_test[y_test == 1]\n",
    "print('Actual Frauds:', len(fruads))\n",
    "non_fruads = y_test[y_test == 0]\n",
    "print('Actual Non-Frauds:', len(non_fruads))\n",
    "\n",
    "#Calculate metrics\n",
    "pfa = cnf_matrix[0][1]/(len(non_fruads))\n",
    "print('Probability of False Alarm:', pfa)\n",
    "pmd = cnf_matrix[1][0]/(len(fruads))\n",
    "print('Probability of Missed Detection', pmd)\n",
    "trpr = cnf_matrix[1][1]/(len(fruads))\n",
    "print('True Positive Rate:', trpr)\n",
    "\n",
    "#Plot confusion matrix\n",
    "fig = plt.figure()\n",
    "plt.imshow(cnf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['Normal', 'Fraud'])\n",
    "plt.yticks(tick_marks, ['Normal', 'Fraud'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.text(0, 0, '{}'.format(cnf_matrix[0,0]), ha='center', va='center', color='white', fontsize=12)\n",
    "plt.text(1, 0, '{}'.format(cnf_matrix[0,1]), ha='center', va='center', color='black', fontsize=12)\n",
    "plt.text(0, 1, '{}'.format(cnf_matrix[1,0]), ha='center', va='center', color='black', fontsize=12)\n",
    "plt.text(1, 1, '{}'.format(cnf_matrix[1,1]), ha='center', va='center', color='black', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "#Precision and Recall and F1 Score\n",
    "recall = recall_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print('Recall:', recall, 'Precision:', precision, 'F1:', f1)\n",
    "\n",
    "#Plot ROC curve\n",
    "y_pred = lr.fit(x_train_u, y_train_u).decision_function(x_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred, drop_intermediate=False)\n",
    "fig = plt.figure()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot(pfa,trpr, 'ro', label = \"Our Model\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('AUC:', auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful code: https://www.kaggle.com/code/joparga3/in-depth-skewed-data-classif-93-recall-acc-now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b9d51fd8e4a399b07b9f8d77bc255b73d146bcbe1df58066270792b4cbaf9eb7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
